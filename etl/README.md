# Background information

The cafe grew and expanded to hundreds of outlets across the country. Due to the demand the company is receiving, they need to figure out how they can best target new and returning customers, and also understand which products are selling well.
They are experiencing issues with collating and analysing the data they are producing at each branch, as their technical setup is limited.

The problems:
- The software currently being used only generates reports for single branches.
- It is time consuming to collate data on all branches.
- Gathering meaningful data for the company on the whole is difficult, due to the limitations of the software.

To resolve the problems, a fully scalable ETL (Extract, Tranform, Load) pipeline was built to handle large volumes of transaction data for the business. This pipeline will collect all the transaction data generated by each individual caf√© and place it in a single location. By being able to easily query the company's data as a whole, the client will drastically increase their ability to identify company-wide trends and insights. 

# How it works

The ETL pipeline will be based on a cloud infrastructure (AWS). 

The CSV files containing raw data of each cafe branch will be uploaded daily to a S3 bucket called "Dirty Data Bucket". 
The Lambda Tranform function will extract the data from each file in the "Dirty Data Bucket" and perform the transformation step and load the tranformed data into a new CSV file in a S3 bucket called "Clean Data Bucket".
The Lambda Load function will load the data in the "Clean Data Bucket" into a Redshift Database.

Grafana is the data analytics software. It is hosted on an EC2 instance using Docker and uses the Redshift Database (mentioned above) as a data source to create Business Intelligence analytics dispayed on a live dashboard that auto-update when new data from new CSV files uploaded that will help the client to gain more insights into their sales trend and branch performance. 

# Folder explanation

This folder contains:

    - Lambda_function: The codes on AWS Lambda that perform the ETL steps 
        - Lambda_load_Redshift: The codes that load the data in the "Clean Data Bucket" into a Redshift Database.
        - Lambda_tranform: the codes that extract the data from each file in the "Dirty-Data" Bucket and perform the transformation step and load the tranformed data into a new CSV file in a S3 bucket called "Clean-Data" Bucket.

    - Tranform: The codes that perform Extract and Tranform steps which were written to work on local machine.

